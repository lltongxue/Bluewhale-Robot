底层架构设计
每个节点不仅要向外发请求还要响应别人的请求。响应的时间可能会比较长。所以要做成异步的模式。
所有的请求和响应都是通过tox网络发出去的，这个和http请求还不太一样。所以要加一些东西来区分出每个请求对应的每个响应。
还要考虑消息有长度限制，底层要自动的实现分包

request
{
	url: the target resource
	method: get, post, update, delete, etc
	uuid: the uuid of the request
	from: from toxid,
	to: to toxid,
	content: msg content
}

分包

{
	id: uuid
	type: start, content, end
	content: package content
}

遇到 type == start, 在包Cache里面新加一个


2015年8月22日09:17:11
非常好第一次通信成功


Requset

from 

node uuid
toxid
request uuid

一个tox client 可以有多个node， 每个node都有唯一的一个id。client是属于底层的结构不对上层的app层开放。
app层至多能看到node层

request url的结构定义

底层 tox client 层， 比如handshake，貌似没必要吧
url: tox/[actions]

node层， 比如添加父级节点
url: node/[data entities]
	风格上和 restful api 一致， 比如添加父级节点实际上就是 post 到 node/childNodes。也就是在对方的子节点
	加上自己的记录


节点的变化
1.节点的互换
	节点优化的主要过程
	特点： 发生时机是可控的
2.节点的新增
	新节点加入网络
	特点： 发生时机随机不可控
3.节点的删除
	节点退出网络
	特点发生时机随机不可控

当有节点下线时应进行的操作
1. 把此节点从自己的数据中删除，同时删除toxcore friend记录
2. 所有可能的情况如下
	1.父节点下线
	从兄弟节点中按照顺序选择一个作为父节点。但是此时目标兄弟节点可能子节点个数已经满了。那就走一般的加入节点流程。这样出
	来的结果可能是很随机的。但是这并没有什么影响。节点结构可以在以后进行优化。
	通知其子节点自己的父节点发生了变化
	2.子节点下线
	通知其他子节点，自己的子节点发生了变化。删除相应的记录及toxcore friend记录
	3.父父节点下线
	删除记录
	4.兄弟节点下线
	删除记录

总觉得这里的lock会出问题，比如被人锁上了，但是对方又没有解锁，然后挂了。这就会一直
都被锁着。但是不锁着也是不行的，很多人改同一个东西就全乱完了。

refactor



相对于zeronet的优势是什么？

1.真正的p2p,不用专门改路由向外暴露端口
2.不只是web，更广泛的app框架
3.数据同步应该会更快。
4.对大文件的支持不行。在文件处理上这个做的比较简单。只是单纯的整体文件发送。所以即使只是改了一个文件中的很小的一部分。整个文件都会被重新下载。
他的数据库实现应该就是把数据库文件做了同步。

对于ceph的总结
1.基本结构
存储结点
客户端
监控

数据同步策略
数据是存在PG里面，也就是一系列客户端的集合。PG是一个有序的存储结点列表。在做数据操作的时候优先操作第一个节点。这个节点被称作基础节点。之后的同步就由这个节点开始了。由于起始节点是确定的，所以同步起来还是比较容易的。但是这样就没有最大限度地发挥出所有节点的优势。主节点的负担太重了。


值得参考的是从数据对象映射PG，PG节点映射的这个过程。CRUSH算法值得研究一下。
节点的状态控制也是值得参考的。in,out,on,off这几个状态。

我那个设计也是不太好。主要问题就是节点之间需要通过n多节点才能直接通信。
我的数据肯定不是每个节点都存一份的。对于很大的数据这样耗费的空间太多了。这样就存在数据的冗余和分布的问题。

出现问题的总是比较极限的情况

现在的主要问题就是当上一个更改尚未同步完全时，又发生了新的更改。ceph由于操作的节点是一个所以只要增序记录下来更改的内容就行了。但是我这个就不是这样。肯定要有merge之类的东西。
实际上并不需要保证每次都读出来的是最新的内容。但是要保证每次读出来的是正确的内容。ceph通过主节点的增序记录就能保证这一点(而且还是在不加锁的情况下)。但是我的merge就没办法保证这一点。我的这个只能通过加锁保证正确性。

文件被分割成最大50M的小块。我不想分的太多。因为分的越多那么想要还原文件涉及到的文件块就越多。这样就越可能出现问题。

文件的分配策略。首先把文件分成文件小块。运行和此文件相关的应用的。慢着，文件应该和应用独立起来。每个文件块有这个文件块对应的存储组。如果节点访问了这个文件，那么就自动加入这个文件的 存储组。

文件传递利用tox自带的文件传递函数。首先发送方发送文件相关的信息，接收方返回一个uuid作为文件标记。然后发送方开始传递文件并用这个uuid进行标记。接收方根据uuid对文件进行处理。

文件存储在什么地方？
在本地 用文件的uuid做文件名，最好不要存在同一个文件夹里面，这个文件比较多的时候对性能有影响。

文件的新增
    类似于做种的过程。
    接口
        node.createFile('filename', 'content'), return the file uuid.
        node.createFile('local file name'), create file from local file. return the file uuid
    过程
        把文件按照存储规则存在本地。如果是直接从本地创建的，那么就不需要再另外存储了。
        在本地数据库添加文件记录。
        当收到文件请求的时候，根据请求信息返回文件数据

文件删除

文件查找
    那就是直接利用广播了。广播文件名信息，然后各个节点进行响应。

文件的更新

文件下载
    url: node/node-uuid/file/file-uuid
    method: get
    # progress and speed are needed.
    收到文件后要自己验证一下sha值对不对
    下载后就要加入文件的网络了， // 可能有问题

文件的读取和更新原则
不保证读到的一定是最新的文件。但是保证读到的一定是正确的文件。所谓正确就是这个文件一定是在某个历史版本中。不会错过历史更改的内容。

所有和文件的操作请求中都会包含这个文件存储群的状态的信息。

MIT的CPU缓存同步策略好像和这个还是略有不同。那个operator的timestamp实际上就是相当于给这些节点排了个顺序。
这样每次读取数据就要先找这个数据的控制节点同步一次。不就相当于直接读取控制节点的数据了吗。这样缓存还有什么意义。
并不是每一次读取都向operator进行同步。只是第一次读取的时候进行同步。以后其他节点更新的时候就要通知这个operator。

我的更新策略
节点有两种状态1.最新，2.更新中，所以只要保证更新发生在最新的节点就可以了。因为最新的节点内容是一样的。最新的节点只能有一个。这也就相当于是个锁。
具体过程。
以A,B,C,D四个节点为例
A，B，C，D，都是最新状态
A先要更新内容。A向所有最新节点加锁。
A更新内容 //这个过程发生在A的本地，时间上基本可以忽略。
A把其他节点标记成更新中
A把最新内容推送到其他节点

如果两个节点同时更新内容
    两个节点同时向外发送加锁信息，这样就乱了。所以这种设计是不行的。只能有一个最新节点。
如果在此同时其他节点想要更新内容
如果其他同时其他节点想要获取内容

更新策略
还是以A，B，C，D，四个节点为例
A是基本节点
A想要更新内容。A直接进行更新

其他节点想要更新内容。
先把A给锁住，然后确认已经和A更新到最新内容。
把自己标记成主节点
把A给解锁
数据同步到其他节点

这里的区别就很明显了。MIT的这个方法是会更换主节点的。这个就比较适合像CPU这种节点比较稳定的结构。但是对于p2p网络，节点随时都可能下线。主节点不能这样随意更换。

我的网络结构设计中天然的就有一个主节点。所以对于我的网络结构具体过程应该如下
还是你A，B，C，D，四个节点
这些节点形成网络结构 
某个节点想要更新信息， 先把最高的父节点给锁住
和最高的父节点进行同步
把文件内容更新到最高父节点
解锁
最高父节点把更新同步到其他节点

如果同时有两个节点想要更新内容
    这就看谁先把主节点给锁了。
如果此时其他节点想要获取内容

如果此时其他节点想要更新内容

对于建立连接的问题
#每个文件都要有自己的一组网络吗？如果这样对于一个1000page的网站，如果每页的节点有100个。每个网络需要建立的链接就是十个左右。就要维建立10000个连接，这显然是不对的。
这里发现和ceph的环境又不太一样。ceph应该更适合局域网。所以节点间没有必要建立长链接。但是tox自然的就会建立长连接。
node在很大程度上是重复的。很多node应该会共享tox连接。比如只有两台机器去host这个网站那么无论有多少节点连接应该也只有一个。所以给每个文件建立网络应该是可行的。
不应该给每个文件都建一个网络，我这个网络就相当于ceph的place group.为了减少网络数量可以几个文件放在一个place group里面。当然这几个文件的相关性要足够高。这个应该由网络自己去决定。

还是要给每个文件都建立网络。因为文件的存储原则上是你用到了这个文件，你就有存储共享的责任，反之则无。如果分成文件group每个人就要存储一些自己不需要的文件。
为了解决这个问题可以把文件分成静态文件和动态文件。只对动态文件建立网络。静态文件只是下载而已。


为什么不全部使用一个网络？
因为这样每个节点就要处理很多和自己不相关的东西。网络比较多我感觉不停的加入网络，处理网络的变化会很耗资源。

对于一个1000个页面的网站，在浏览的时候没必要把所有的网站内容都下载下来。应该是用到什么内容下载什么内容。所以实际浏览这个网站的时候并不会建立很多连接。

需要有一个http的文件接口
获取一个文件
http://127.0.0.1/node/node-uuid/file/sha
    过程
        检查本地网络是否有此文件记录

这样也还是有一点不太好，比如即使内容一样的文件可能也还是会有不同的uuid。这样就会浪费存储资源。可以用sha值作为文件的标识。
创建文件的过程要进行修改
1. 计算sha值，在本地数据库添加文件记录
2. 检查是否网络中已经存在此文件
3. 如果存在，则加入此文件对应的node网络，反之不存在则创建新的node网络。

启动过程。检查本地有哪些文件记录。加入每个文件的网络。


如何进行文件的检索
1. 利用sha值进行检索
    所有的tox节点都必须加入一个node网络。通过此网络对文件检索进行广播查询。
2. 根据文件名进行检索
    同上

问题，为什么文件要进行分割？
减少文件占用的空间，把一个文件的不同内容存储在不同的节点上，这样每个节点所需要的存储空间就会小很多。
这个应该以后进行考虑，现阶段不对文件进行分割，同样文件的传递策略现阶段也不做深入考虑。因为一旦分割，利用sha值作为标识处理起来就会更复杂一些。

url format
/api/tox/{tox-id}/node/{node-id}/entity

node request format

public string url { get; set; }
public string method { get; set; }
public string uuid { get; set; }
public string content { get; set; }
public string fromNodeId { get; set; }
public string fromToxId { get; set; }
public string toNodeId { get; set; }
public string toToxId { get; set; }

http request format // I donot want to miss any info from node request

url
method
content
Request-Uuid
From-Tox-Id
From-Node-Id


现在的问题，由于开了好几个监听，怎么确定req 来自那个监听的程序？

处理自己给自己发信息的情况 直接通过 RequestProxy 把请求转到本地

添加各种controllers


先不考虑各种优化的问题，把整体结构实现再说。
要保证随时有节点加入和离开网络都都不会使网络出现问题。
这里锁就是一个比较大的问题，如果对方掉线而且还锁住了自己，这样就会一直锁下去。下线了就自动解锁


在 ToxRequest 对象里面添加 time 属性。在更改数据的时候只有 time 大于当前值才进行更改。
由于请求都是在父节点进行处理的，这里不会存在数据混乱的情况

TODO:
add testcase
add request Time Check
